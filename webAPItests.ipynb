{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-up & Main Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import threading\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from collections import Counter\n",
    "from typing import Any, Callable, Tuple\n",
    "import random\n",
    "\n",
    "import WebAPINotes.cpwalib as cpwalib\n",
    "import WebAPINotes.endpoints as endpoints\n",
    "import WebAPINotes.barFuncs as barFuncs\n",
    "import WebAPINotes.pms as pms\n",
    "import WebAPINotes.tickler as tickler\n",
    "from WebAPINotes.barFuncs import Bars\n",
    "\n",
    "import TechnicalAnalysis.techInds as Indicators\n",
    "\n",
    "from config import globals\n",
    "from config.validFields import *\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set log level to INFO\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Include timestamp\n",
    "    handlers=[\n",
    "        logging.FileHandler(globals.status_file, mode='a')  # Append to the log file\n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "tickle_thread = threading.Thread(target=tickler.tickle)\n",
    "tickle_thread.start()\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Fixed API Data Tables to initialize as env variables \"\"\"\n",
    "# html tables copied from official documentation: may need update\n",
    "ftdir: str = \"WebAPINotes/fixedDataTables\"\n",
    "\n",
    "mdf: pd.DataFrame = pd.read_html(os.path.join(ftdir, \"market_data_fields.html\"))[0]\n",
    "vbu: pd.DataFrame = pd.read_html(os.path.join(ftdir, \"valid_bar_units.html\"))[0]\n",
    "vpu: pd.DataFrame = pd.read_html(os.path.join(ftdir, \"valid_period_units.html\"))[0]\n",
    "osv: pd.DataFrame = pd.read_html(os.path.join(ftdir, \"order_status_values.html\"))[0]\n",
    "spm: pd.DataFrame = pd.read_html(os.path.join(ftdir, \"suppressible_messages.html\"))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contract Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_contract = cpwalib.contractSearch(\n",
    "    {\n",
    "        \"symbol\": \"BTC\",\n",
    "        \"name\":   False,\n",
    "    }\n",
    ")\n",
    "\n",
    "req_contract.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved Contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "DAX INDEX\n",
    "\n",
    "{'conid': '825711',\n",
    "  'companyHeader': 'DAX 40 Index (Deutsche Aktien Xchange 40) - EUREX',\n",
    "  'companyName': 'DAX 40 Index (Deutsche Aktien Xchange 40)',\n",
    "  'symbol': 'DAX',\n",
    "  'description': 'EUREX',\n",
    "  'restricted': 'IND',\n",
    "  'fop': None,\n",
    "  'opt': '20240812;20240813;20240814;20240815;20240816;20240823;20240830;20240906;20240913;20240920;20240930;20241018;20241031;20241220;20250321;20250620;20250919;20251219;20260320;20260619;20260918;20261218;20270319;20270618;20271217;20281215',\n",
    "  'war': '20240711;20240712;20240715;20240716;20240717;20240718;20240719;20240722;20240723;20240724;20240725;20240726;20240729;20240730;20240731;20240801;20240802;20240805;20240806;20240807;20240808;20240809;20240812;20240813;20240814;20240815;20240816;20240819;20240820;20240821;20240822;20240823;20240826;20240829;20240830;20240902;20240903;20240905;20240906;20240909;20240912;20240913;20240916;20240917;20240918;20240919;20240920;20240923;20240926;20240927;20240930;20241001;20241003;20241004;20241007;20241010;20241011;20241014;20241015;20241016;20241017;20241018;20241025;20241030;20241101;20241105;20241108;20241112;20241113;20241114;20241115;20241122;20241129;20241203;20241213;20241216;20241217;20241218;20241219;20241220;20241227;20250114;20250116;20250117;20250218;20250220;20250221;20250314;20250317;20250318;20250320;20250321;20250415;20250417;20250513;20250515;20250616;20250617;20250619;20250620;20250715;20250812;20250915;20250916;20250918;20250919;20250929;20251014;20251118;20251216;20251218;20251219;20260319;20260618',\n",
    "  'sections': [{'secType': 'IND', 'exchange': 'EUREX;'},\n",
    "\n",
    "\n",
    "DAX FUT\n",
    "\n",
    "{'conid': 568953467,\n",
    "  'symbol': 'DAX',\n",
    "  'secType': 'FUT',\n",
    "  'exchange': 'EUREX',\n",
    "  'listingExchange': 'EUREX',\n",
    "  'right': '?',\n",
    "  'strike': 0.0,\n",
    "  'currency': 'EUR',\n",
    "  'cusip': None,\n",
    "  'coupon': 'No Coupon',\n",
    "  'desc1': \"Jun20'25(5)\",\n",
    "  'desc2': None,\n",
    "  'maturityDate': '20250620',\n",
    "  'multiplier': '5',\n",
    "  'tradingClass': 'FDXM',\n",
    "  'validExchanges': 'EUREX'}\n",
    "\n",
    "DAX STK\n",
    "\n",
    "{'conid': '346727821',\n",
    "  'companyHeader': 'GLOBAL X DAX GERMANY ETF - NASDAQ',\n",
    "  'companyName': 'GLOBAL X DAX GERMANY ETF',\n",
    "  'symbol': 'DAX',\n",
    "  'description': 'NASDAQ',\n",
    "  'restricted': None,\n",
    "  'fop': None,\n",
    "  'opt': '20240816;20240920;20241018;20250117',\n",
    "  'war': None,\n",
    "  'sections': [{'secType': 'STK'},\n",
    "   {'secType': 'OPT',\n",
    "    'months': 'AUG24;SEP24;OCT24;JAN25',\n",
    "    'exchange': 'SMART;AMEX'},\n",
    "   {'secType': 'BAG'}]}]\n",
    "\n",
    "\n",
    "TSLA\n",
    "\n",
    "{'conid': '76792991',\n",
    "  'companyHeader': 'TESLA INC - NASDAQ',\n",
    "  'companyName': 'TESLA INC',\n",
    "  'symbol': 'TSLA',\n",
    "  'description': 'NASDAQ',\n",
    "  'restricted': None,\n",
    "  'fop': None,\n",
    "  'opt': '20240816;20240823;20240830;20240906;20240913;20240920;20240927;20241018;20241115;20241220;20250117;20250221;20250321;20250620;20250815;20250919;20251219;20260116;20260618;20261218',\n",
    "  'war': '20240711;20240712;20240715;20240716;20240717;20240718;20240719;20240723;20240724;20240725;20240726;20240729;20240730;20240731;20240801;20240802;20240805;20240806;20240807;20240808;20240809;20240813;20240815;20240816;20240823;20240830;20240906;20240913;20240917;20240918;20240919;20240920;20241017;20241018;20241114;20241115;20241213;20241216;20241217;20241218;20241219;20241220;20250114;20250115;20250116;20250117;20250220;20250314;20250318;20250319;20250320;20250321;20250613;20250617;20250618;20250619;20250620;20250918;20250919;20251216;20251218;20251219;20260113;20260114;20260115;20260116;20260319;20260616;20260617;20260618;20260619;20260917;20261217;20261218;20270112;20270114;20270616;20270617;20271216;20271217;20280615;20280616',\n",
    "  'sections': [{'secType': 'STK'}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contract Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the info to get specific contract\n",
    "req_contract_info = cpwalib.contractInfo(\n",
    "    {\n",
    "        \"conid\": \"346727821\", # for STK/IND conid ONLY (default), others requrie ALL other 3 params\n",
    "        \"secType\": \"STK\",  \n",
    "        #\"month\": \"JUN25\",\n",
    "        #\"exchange\": \"EUREX\",\n",
    "        \n",
    "    }\n",
    ")\n",
    "\n",
    "req_contract_info.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contract Strike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an option, and check all the strike prices available:\n",
    "req_strike_info = cpwalib.contractStrikes(\n",
    "    {\n",
    "        \"conid\": \"825711\",\n",
    "        \"secType\": \"OPT\",\n",
    "        \"month\": \"JUL24\",\n",
    "        \"exchange\": \"EUREX\",\n",
    "        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_strike_info.json().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(req_strike_info.json()[\"put\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick some desired strike price and feed it back into contract info search for options contract\n",
    "req_options_info = cpwalib.contractInfo(\n",
    "    {\n",
    "        \"conid\": \"825711\",\n",
    "        \"secType\": \"OPT\",\n",
    "        \"month\": \"JUL24\",\n",
    "        \"exchange\": \"EUREX\",\n",
    "        \"strike\": req_strike_info.json()[\"put\"][70],\n",
    "        \"right\": \"P\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fetch the desired option:\n",
    "req_options_info.json()[3][\"multiplier\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure alert to be sent whenever order for given contract is placed\n",
    "conidex: str = \"346727821\"\n",
    "msg: str = \"DAX ETF Alert Test 2\"\n",
    "\n",
    "json_content: dict = {\n",
    "    \"alertName\": \"MTA Time Alert 4\",\n",
    "    \"alertMessage\": msg,\n",
    "    \"alertRepeatable\": 0,\n",
    "    \"email\": \"mezzacapa01@outlook.com\",\n",
    "    #\"expireTime\": \"20231231-12:00:00\",\n",
    "    # \"iTWSOrdersOnly\": 0,\n",
    "    \"outsideRth\": 1,\n",
    "    \"sendMessage\": 1,\n",
    "    # \"showPopup\": 1,\n",
    "    \"tif\": \"GTC\",\n",
    "    \"conditions\": [\n",
    "    {\n",
    "        \"conidex\": conidex,\n",
    "        \"logicBind\": \"n\",\n",
    "        \"operator\": \">=\",\n",
    "        \"triggerMethod\": 0,\n",
    "        \"type\": 3, \n",
    "        \"value\": '20240812-14:05:00'\n",
    "    }\n",
    "    ]\n",
    "}\n",
    "\n",
    "resp_al = cpwalib.createAlert(globals.AccountID.FABIO.value, json_content)\n",
    "\n",
    "resp_al.text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_al = cpwalib.getAlerts(globals.AccountID.FABIO.value)\n",
    "\n",
    "resp_al.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Market Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Market Data Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to find field based on query substr\n",
    "def find_mkdf(mdf: pd.DataFrame, query_val: str) -> pd.DataFrame:\n",
    "    return mdf[mdf[\"Value\"].apply(lambda val: query_val.lower() in val.lower())]\n",
    "\n",
    "find_mkdf(mdf, \"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now locate desired field\n",
    "mdf.loc[0, \"Field\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Market Data Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "The endpoint /iserver/accounts must be called \n",
    "prior to /iserver/marketdata/snapshot.\n",
    "\"\"\"\n",
    "\n",
    "acc_resp = requests.get(endpoints.base_url + endpoints.accounts, verify=False)\n",
    "\n",
    "acc_resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_mds = cpwalib.marketDataSnapshot(\n",
    "    #conids=['540729681', '673277361', '540729524'],\n",
    "    conids=['825711'],\n",
    "    fields=[7295, 7296, 70, 71, 87]\n",
    ")\n",
    "\n",
    "# First request does not give much out, other than repeating conids: may need further reqs\n",
    "req_mds.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Historical Data 1 (/hdms endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "/hdms endpoint seems dead in general (BETA feature) \n",
    "\"\"\"\n",
    "req_hd = cpwalib.historicalData(\n",
    "    conid=\"825711\",\n",
    "    period=1,\n",
    "    period_unit=PeriodUnits.DAY,\n",
    "    bar=1,\n",
    "    bar_unit=BarUnits.MINUTE,\n",
    "    outsideRth=False,\n",
    "    startTime=None,\n",
    "    direction=PeriodDirecion.START_TO_NOW,\n",
    "    barType=BarType.MIDPOINT\n",
    ")\n",
    "\n",
    "req_hd.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Historical Data 2 (/marketdata/history endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "req_mdh = cpwalib.marketDataHistory(\n",
    "    conid=\"825711\",\n",
    "    period=3,\n",
    "    period_unit=PeriodUnits.YEAR,\n",
    "    bar=1,\n",
    "    bar_unit=BarUnits.DAY,\n",
    "    outsideRth=True,\n",
    "    startTime=None,\n",
    "    exchange=None\n",
    ")\n",
    "\n",
    "req_mdh.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_col: str = \"t\"\n",
    "def mdh_to_df(mdh_data: dict) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(mdh_data)\n",
    "    # Then convert to timestamp objects (considering millisecond units)\n",
    "    df.t = df.t.apply(lambda unix_time: pd.Timestamp(unix_time, unit='ms'))\n",
    "    # Ensure we are sorting by increasing date \n",
    "    df = df.sort_values(by=time_col)\n",
    "    return df\n",
    "\n",
    "mdh_to_df(req_mdh.json().get(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perpetual updating algorithm\n",
    "conid: str = \"825711\"\n",
    "periods: int = 1\n",
    "period_unit = PeriodUnits.DAY\n",
    "bar : int = 1\n",
    "bar_unit = BarUnits.MINUTE\n",
    "update_time: int = 120\n",
    "max_loops: int = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize ordered data frame\n",
    "curr_mdh:  pd.DataFrame = mdh_to_df(\n",
    "    cpwalib.marketDataHistory(conid=conid, period=periods, period_unit=period_unit, bar=bar, bar_unit=bar_unit).json().get(\"data\")\n",
    ")\n",
    "\n",
    "# Enter in recurring loop (needs threading)\n",
    "curr_it: int = 0\n",
    "while curr_it < max_loops:\n",
    "    # Get freshly updated, sorted dataframe\n",
    "    new_mdh: pd.DataFrame = mdh_to_df(\n",
    "        cpwalib.marketDataHistory(conid=conid, period=periods, period_unit=period_unit, bar=bar, bar_unit=bar_unit).json().get(\"data\")\n",
    "    )\n",
    "    # Iterate through new dataframe, adding each row if more recent than most recent (assumes that both dataframes are already sorted)\n",
    "    for index, row in new_mdh.iterrows():\n",
    "        if row[time_col] > curr_mdh[time_col].iloc[-1]:\n",
    "            curr_mdh = pd.concat([curr_mdh, pd.DataFrame(row)], ignore_index=True)\n",
    "    # Sleep until next cycle\n",
    "    time.sleep(update_time)\n",
    "    curr_it += 1\n",
    "\n",
    "curr_mdh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BBG bar extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw: pd.DataFrame = pd.read_excel('dax daily.xlsx', header=6)\n",
    "\n",
    "ref = raw[[\"Date\", \"PX_OPEN\", \"PX_LAST\", 'PX_LOW', 'PX_HIGH']]\n",
    "\n",
    "key_remaps: dict = {\n",
    "    \"Date\": Bars.fields.DATE.value,\n",
    "    \"PX_OPEN\": Bars.fields.OPEN.value,\n",
    "    \"PX_LAST\": Bars.fields.CLOSE.value,\n",
    "    \"PX_LOW\": Bars.fields.LOW.value,\n",
    "    \"PX_HIGH\": Bars.fields.HIGH.value,\n",
    "}\n",
    "\n",
    "ref = ref.rename(columns=key_remaps)\n",
    "#ref.to_excel(\"dax_daily4.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IBKR extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "Full procedure to extract hourly bars for previous month\n",
    "\"\"\"\n",
    "\n",
    "mdh: pd.DataFrame = Bars.reqBars(\n",
    "    conid = \"825711\",\n",
    "    periods = 1800,\n",
    "    period_unit = PeriodUnits.SECOND,\n",
    "    bar = 1,\n",
    "    bar_unit = BarUnits.SECOND,\n",
    "    outsideRth = False,\n",
    ")\n",
    "\n",
    "mdh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time delta minimum date difference\n",
    "\n",
    "\n",
    "# if mdh.empty or Bars.fields.TIME.value not in mdh.index: continue\n",
    "target_bar_size = pd.Timedelta(minutes=4)\n",
    "\n",
    "input_bar_size : pd.Timedelta = mdh[Bars.fields.TIME.value].apply(lambda d: pd.to_datetime(d, unit=globals.UNIX_TIME_UNITS)).diff().min()\n",
    "agg_fact: int = int(target_bar_size.total_seconds() / input_bar_size.total_seconds())\n",
    "aggd_bars: pd.DataFrame= Bars.aggBars(\n",
    "    mdh=mdh,\n",
    "    agg_fact = agg_fact,\n",
    "    bar_size = input_bar_size,\n",
    ")\n",
    "\n",
    "aggd_bars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_bars: pd.DataFrame= Bars.aggBars(\n",
    "    mdh=mdh,\n",
    "    agg_fact = 2,\n",
    "    bar_size = pd.Timedelta(minutes=30),\n",
    ")\n",
    "\n",
    "hourly_bars.to_excel(\"Dax hourly bars.xlsx\")\n",
    "\n",
    "hourly_bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Excel Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bars: pd.DataFrame = pd.read_excel(\"Dax daily bars.xlsx\").drop(columns=[\"Unnamed: 0\"])\n",
    "test_bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Local Stationary Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some bar aggregate \n",
    "from config.validFields import BarFields\n",
    "from TechnicalAnalysis.techInds import BarAgg\n",
    "barClose: BarAgg = lambda bar: bar[BarFields.CLOSE.value] # instance definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Local Stationary Points \"\"\"\n",
    "import TechnicalAnalysis.techInds as Indicators\n",
    "\n",
    "# Function parameters\n",
    "m = Indicators.LocStat.typ.MAX \n",
    "bar_agg = lambda i: barClose(bars.loc[i])\n",
    "\n",
    "stat_pts_df = Indicators.LocStat().compute(bars=hourly_bars, m=m, barAgg=barClose)\n",
    "stat_pts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stat_pts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive method (verification)\n",
    "stat_list: List[int] = []\n",
    "for i in range(1, len(hourly_bars) - 1):\n",
    "    stat_list.append(i) if m.value*bar_agg(i) < m.value*bar_agg(i+1) and m.value*bar_agg(i) < m.value*bar_agg(i-1) else None\n",
    "pd.Series(stat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "daily_bars: pd.DataFrame= Bars.reqBars(\n",
    "    conid = \"825711\",\n",
    "    periods = 1,\n",
    "    period_unit = PeriodUnits.YEAR,\n",
    "    bar = 1,\n",
    "    bar_unit = BarUnits.DAY,\n",
    "    outsideRth = False,\n",
    ")\n",
    "\n",
    "daily_bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RSI \"\"\"\n",
    "\n",
    "RSI_pts: pd.DataFrame = Indicators.RSI().compute(\n",
    "    bars=test_bars,\n",
    "    period=14,\n",
    "    barAgg=lambda bar: bar[Bars.fields.CLOSE.value]\n",
    ")\n",
    "\n",
    "RSI_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hourly_bars: pd.DataFrame = pd.read_excel(\"DAX_hourly_testbars.xlsx\")\n",
    "hourly_bars.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "hourly_bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bollinger Bands & Crossings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Bollinger Bands \"\"\"\n",
    "import TechnicalAnalysis.techInds as Indicators\n",
    "BB_data : pd.DataFrame = Indicators.BollBands.compute(\n",
    "    bars=hourly_bars, \n",
    "    period=20,\n",
    "    mult=2.0,\n",
    "    barAgg= lambda bar: bar[BarFields.CLOSE.value],\n",
    ")\n",
    "\n",
    "BB_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Bollinger crossings \"\"\"\n",
    "band = Indicators.BollBands.fields.BOLL_UPPER\n",
    "dir = Indicators.BollBands.crossDir.UP\n",
    "\n",
    "cross = lambda i: Indicators.BollBands.cross(\n",
    "    bars=hourly_bars, \n",
    "    BollBands=BB_data,\n",
    "    barAgg=lambda bar: bar[BarFields.CLOSE.value],\n",
    "    index=i,\n",
    "    band=band,\n",
    "    dir=dir,\n",
    ")\n",
    "\n",
    "cross_indeces   = [i for i in hourly_bars.index if cross(i)]\n",
    "cross_dates     = [hourly_bars.loc[i][BarFields.DATE.value] for i in hourly_bars.index if cross(i)]\n",
    "cross_dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Divergence \"\"\"\n",
    "import TechnicalAnalysis.techInds as Indicators\n",
    "\n",
    "# Constant parameters\n",
    "barClose: Indicators.BarAgg = lambda bar: bar[Bars.fields.CLOSE.value]\n",
    "\n",
    "# Function arguments\n",
    "max_div_period = 14\n",
    "max_neg_period=14\n",
    "#m=Indicators.LocStat.typ.MIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main compute method\n",
    "Div_res = Indicators.Div.compute(\n",
    "    test_bars, \n",
    "    barClose, \n",
    "    RSI_period=14, \n",
    "    max_div_period=max_div_period, \n",
    "    max_neg_period=max_neg_period\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Div_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process df before saving back results to excel\n",
    "Div_res[Indicators.Div.fields.LBP.value] = Div_res[Indicators.Div.fields.LBP.value].apply(lambda i: test_bars.loc[i][Bars.fields.DATE.value])\n",
    "Div_res[Indicators.Div.fields.RBP.value] = Div_res[Indicators.Div.fields.RBP.value].apply(lambda i: test_bars.loc[i][Bars.fields.DATE.value])\n",
    "Div_res[Indicators.Div.fields.NEG.value] = Div_res[Indicators.Div.fields.NEG.value].apply(lambda i: test_bars.loc[i][Bars.fields.DATE.value] if i >= 0 else None)\n",
    "Div_res[Indicators.Div.fields.M.value] = Div_res[Indicators.Div.fields.M.value].apply(lambda m: \"max\" if m==-1 else \"min\")\n",
    "\n",
    "Div_res[Indicators.Div.fields.LBP.value] = Div_res[Indicators.Div.fields.LBP.value].apply(lambda t: t.strftime('%d-%m-%Y'))\n",
    "Div_res[Indicators.Div.fields.RBP.value] = Div_res[Indicators.Div.fields.RBP.value].apply(lambda t: t.strftime('%d-%m-%Y'))\n",
    "Div_res[Indicators.Div.fields.NEG.value] = Div_res[Indicators.Div.fields.NEG.value].apply(lambda t: t.strftime('%d-%m-%Y') if pd.notna(t) else '')\n",
    "\n",
    "Div_res.rename(\n",
    "    columns={\n",
    "        Indicators.Div.fields.LBP.value: \"left date\",\n",
    "        Indicators.Div.fields.RBP.value: \"right date\",\n",
    "        Indicators.Div.fields.M.value: \"on max / min\",\n",
    "    }, \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "Div_res.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Div_res.to_excel(\"Dax Divergenze Negate Daily.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtrace testing \n",
    "RBP, LBP = \"right date\", \"left date\"\n",
    "\n",
    "i: int = 28\n",
    "inter_lbps = False\n",
    "div_series_list = [row for _, row in Div_res.iterrows()]\n",
    "\n",
    "def collect_root_lbps(source_rbp: str) -> List[str]:\n",
    "    \"\"\" Returns LBP for each fundamental deg. 1 divergence \"\"\"\n",
    "    in_divs = [pdp for pdp in div_series_list if (pdp[RBP] == source_rbp)]\n",
    "    in_lbp = [pdp[LBP] for pdp in in_divs]\n",
    "    if not in_divs: # Base case: if source_rbp does not stem into other incoming divergences, return this as a root\n",
    "        return [source_rbp]\n",
    "    return [ldp for ldp_list in (collect_root_lbps(lbp) for lbp in in_lbp) for ldp in ldp_list]\n",
    "\n",
    "def collect_successors(source_lbp: str) -> List[str]:\n",
    "    \"\"\"  Returns LBP for each div. stemming and including the div. with source_lbp \"\"\"\n",
    "    out_divs = [pdp for pdp in div_series_list if (pdp[LBP] == source_lbp)]\n",
    "    result = [out_div[LBP] for out_div in out_divs]\n",
    "    for out_div in out_divs:\n",
    "        result += collect_successors(out_div[RBP])\n",
    "    return result\n",
    "\n",
    "\n",
    "collect_root_lbps(Div_res.loc[i][RBP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_root_lbps(\"16-09-2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_successors('05-09-2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ladders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TechnicalAnalysis.techInds as Indicators\n",
    "\n",
    "# Must have defined test_bars first\n",
    "\n",
    "bar_agg = lambda bar: bar[Bars.fields.CLOSE.value]\n",
    "min_stat_pts = 6\n",
    "\n",
    "req_dir = Indicators.Ladders.dir.UP\n",
    "\n",
    "ladders = Indicators.Ladders.compute(\n",
    "    test_bars, \n",
    "    bar_agg, \n",
    "    min_stat_pts=min_stat_pts\n",
    ")\n",
    "ladders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process df before saving back results to excel\n",
    "ladders[Indicators.Ladders.fields.LEFT_STAT.value] = ladders[Indicators.Ladders.fields.LEFT_STAT.value].apply(lambda i: test_bars.loc[i][Bars.fields.DATE.value])\n",
    "ladders[Indicators.Ladders.fields.LEFT_STAT.value] = ladders[Indicators.Ladders.fields.LEFT_STAT.value].apply(lambda t: t.strftime('%d-%m-%Y'))\n",
    "ladders[Indicators.Ladders.fields.RIGHT_STAT.value] = ladders[Indicators.Ladders.fields.RIGHT_STAT.value].apply(lambda i: test_bars.loc[i][Bars.fields.DATE.value])\n",
    "ladders[Indicators.Ladders.fields.RIGHT_STAT.value] = ladders[Indicators.Ladders.fields.RIGHT_STAT.value].apply(lambda t: t.strftime('%d-%m-%Y'))\n",
    "ladders[Indicators.Ladders.fields.DIRECTION.value] = ladders[Indicators.Ladders.fields.DIRECTION.value].apply(lambda m: \"UP\" if m==-1 else \"DOWN\")\n",
    "\n",
    "ladders.rename(\n",
    "    columns={\n",
    "        Indicators.Ladders.fields.LEFT_STAT.value: \"left max/min date\",\n",
    "        Indicators.Ladders.fields.RIGHT_STAT.value: \"right max/min date\",\n",
    "        Indicators.Ladders.fields.DIRECTION.value: \"ladder direction\",\n",
    "    }, \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "ladders.reset_index(drop=True, inplace=True)\n",
    "ladders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ladders.to_excel(\"Test Scale DAX Daily.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main Endpoint Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress every suppressible message\n",
    "req_sm = cpwalib.suppressMessages(spm.MessageId.tolist())\n",
    "req_sm.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.globals import AccountID\n",
    "\n",
    "req_po = cpwalib.placeOrder(\n",
    "    AccountID.FABIO.value,\n",
    "    [\n",
    "        {\n",
    "            \"acctId\": AccountID.FABIO.value,\n",
    "            \"conid\": 346727821,\n",
    "            \"cOID\": \"DAX-ETF-BUY-3\",\n",
    "            \"orderType\": OrderTypes.MARKET.value,\n",
    "            \"side\": OrderSide.BUY.value,\n",
    "            \"tif\": Tif.IOC.value,\n",
    "            \"quantity\": 1,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "req_po.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replying to messages\n",
    "req_or = cpwalib.orderReply(\n",
    "    replyID=req_po.json()[0].get(\"id\"),\n",
    "    confirmed=True\n",
    ")\n",
    "\n",
    "req_or.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check recent orders \n",
    "\n",
    "req_lo = cpwalib.liveOrders(\n",
    "    filters=[\n",
    "        \n",
    "    ],\n",
    "    force=False\n",
    ")\n",
    "\n",
    "lo_df = pd.DataFrame(req_lo.json()[\"orders\"])\n",
    "\n",
    "ttime: int = 240810_0000_00\n",
    "# FIlter to desired columns and only recently executed orders, then sort by time\n",
    "rec_trades = lo_df[lo_df['lastExecutionTime'].astype(int) > ttime][[\"orderId\", \"order_ref\", \"ticker\", \"side\", \"orderType\", \"remainingQuantity\", \"filledQuantity\", \"lastExecutionTime\", \"status\"]].sort_values(by=\"lastExecutionTime\", ascending=False)\n",
    "\n",
    "# Check that filled buys equal filled sells\n",
    "rec_trades[rec_trades[\"ticker\"] == \"MSFT\"][rec_trades[\"side\"] == \"SELL\"][\"filledQuantity\"].sum()\n",
    "\n",
    "rec_trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_lo.json()[\"orders\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check an order status\n",
    "stat_req = cpwalib.orderStatus(\"124496446\")\n",
    "stat_req.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel an order\n",
    "\n",
    "req_co = cpwalib.cancelOrder(globals.accountID, \"\")\n",
    "# req_co.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampling IOC Order Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Assumptions for non-crashing program\n",
    "    (1) bin/run.sh root/conf.yaml script properly running to keep port 5000 open, else tickler.tickle() will throw exception and program cannot run\n",
    "    (2) suppress message request suppresses all potential messages holding order execution; response object ignored\n",
    "    (3) place order response object ignored\n",
    "\"\"\"\n",
    "\n",
    "# Some order constant values\n",
    "conid: int      = 568953467\n",
    "cOID_base: str  = \"DAX-JUN25-IOCretryTest1\"\n",
    "\n",
    "# Define the fields required for the dictionary object\n",
    "order_fields: List[str]=[\n",
    "    \"acctId\", \"conid\", \"cOID\", \"orderType\", \"side\", \"tif\", \"quantity\"\n",
    "]\n",
    "\n",
    "# Define test loop variables\n",
    "num_orders: int = 2                 # How many orders in total\n",
    "verif_time: float = 10.0            # Time between placing IOC order and expecting it to be in live orders output\n",
    "time_per_attempt: float = 10.0      # Time span over which IOCretryTillFilled attempts to fill order\n",
    "max_retries: int = 4                # Maximum retries for IOC, spread over bar_size\n",
    "\n",
    "res: List[dict] = []\n",
    "res_counter = Counter()\n",
    "res_orders: List[Tuple[bool, pd.DataFrame]] = []\n",
    "\n",
    "\"\"\" NOTE: Must suppress messages beforehand, else  \"\"\"\n",
    "cpwalib.suppressMessages(spm.MessageId.tolist())\n",
    "\n",
    "for order_num in range(num_orders):\n",
    "\n",
    "    # Choose random quantity and random side\n",
    "    rand_quantity: int = random.randint(1, 4)\n",
    "    rand_side: str = OrderSide.BUY.value if random.randint(0, 1) == 0 else OrderSide.SELL.value\n",
    "\n",
    "    # Try to fill the order with IOCretryTillFilled \n",
    "    res_val, order_resp = pms.LTPM.IOCretryTillFilled(\n",
    "        globals.accountID,\n",
    "        {\n",
    "            \"acctId\": globals.accountID,\n",
    "            \"conid\": conid,\n",
    "            \"cOID\": f\"{cOID_base}-{order_num}\",\n",
    "            \"orderType\": OrderTypes.MARKET.value,\n",
    "            \"side\": rand_side,\n",
    "            \"tif\": Tif.IOC.value,\n",
    "            \"quantity\": rand_quantity\n",
    "        },\n",
    "        time_per_attempt,\n",
    "        max_retries,\n",
    "        verif_time\n",
    "    )\n",
    "\n",
    "    res.append({\n",
    "        \"side\":         rand_side,\n",
    "        \"req_quantity\": rand_quantity,\n",
    "        \"filled_quant\": order_resp[OrderFields.FILLED_QUANTITY.value].sum() if not order_resp.empty else 0.0\n",
    "    })\n",
    "    res_counter[f'{res_val}'] += 1\n",
    "    res_orders.append((res_val, order_resp))\n",
    "\n",
    "res_df = pd.DataFrame(res)\n",
    "print(res_counter)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing resulting order dataframes\n",
    "ro_cop = copy.deepcopy(res_orders)\n",
    "\n",
    "# Redue dataframes to only rows with some fill value\n",
    "non_empty_orders: List[pd.DataFrame] = [order_resp_tup[1][order_resp_tup[1][OrderFields.FILLED_QUANTITY.value] != 0] for order_resp_tup in ro_cop]\n",
    "\n",
    "# Check the attempts and order count distributions\n",
    "ord_attempts = Counter(ord_seq.iloc[0][OrderFields.ORDER_REF.value][-1] for ord_seq in non_empty_orders)\n",
    "ordno_count = Counter(ord_seq.shape[0] for ord_seq in non_empty_orders)\n",
    "\n",
    "# Display some seletc columns\n",
    "non_empty_orders[41][[OrderFields.FILLED_QUANTITY.value, OrderFields.ORDER_REF.value, OrderFields.STATUS.value, OrderFields.ORDERID.value]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Live Trading PM tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Open trade tests\n",
    "Assumptions for non-crashing program\n",
    "    (1) bin/run.sh root/conf.yaml script properly running to keep port 5000 open, else tickler.tickle() will throw exception and program cannot run\n",
    "    (2) place order response object ignored\n",
    "\"\"\"\n",
    "\n",
    "from WebAPINotes.pms import Ltpm\n",
    "\n",
    "# Some order constant values\n",
    "conid: int      = 265598\n",
    "cOID_base: str  = \"AAPL-TradingTest7\"\n",
    "\n",
    "# Define the fields required for the dictionary object\n",
    "order_fields: List[str]=[\n",
    "    \"acctId\", \"conid\", \"cOID\", \"orderType\", \"side\", \"tif\", \"quantity\"\n",
    "]\n",
    "\n",
    "# Define test loop variables\n",
    "num_trades: int = 6\n",
    "\n",
    "# Initialize LTPM object\n",
    "pm = Ltpm(\n",
    "    acctId=globals.AccountID.FABIO.value,\n",
    "    json_filedir=\"trade_data.json\",\n",
    "    time_per_attempt=10.0,\n",
    "    max_retries=4,\n",
    "    verif_time=10.0,\n",
    "    spm=spm.MessageId.tolist()\n",
    ")\n",
    "\n",
    "# Attempt to initialize trade data from file if possible\n",
    "pm.load_json()\n",
    "\n",
    "trade_indexes: List[int] = []\n",
    "\n",
    "for trade_num in range(num_trades):\n",
    "    # Choose random quantity and random side\n",
    "    rand_quantity: int = random.randint(1, 4)\n",
    "    rand_side: str = OrderSide.BUY.value if random.randint(0, 1) == 0 else OrderSide.SELL.value\n",
    "\n",
    "    # Define order to be passed to open trade\n",
    "    req_open_order: dict = {\n",
    "        \"acctId\": globals.AccountID.FABIO.value,\n",
    "        \"conid\": conid,\n",
    "        \"cOID\": f\"{cOID_base}-{trade_num}\",\n",
    "        \"orderType\": OrderTypes.MARKET.value,\n",
    "        \"side\": rand_side,\n",
    "        \"tif\": Tif.IOC.value,\n",
    "        \"quantity\": rand_quantity\n",
    "    }\n",
    "\n",
    "    # Simulate bar movement\n",
    "    last_bar = Bars.reqBars(\n",
    "        conid = conid,\n",
    "        periods = 1,\n",
    "        period_unit = PeriodUnits.MONTH,\n",
    "        bar = 1,\n",
    "        bar_unit = BarUnits.HOUR,\n",
    "        outsideRth = False,\n",
    "    ).iloc[trade_num + (trade_num % 2)] # Simulate bar repetition\n",
    "\n",
    "    # Attempt to open trade with this order\n",
    "    trade_index: int = pm.openTrade(\n",
    "        bar=last_bar, \n",
    "        reason=f\"Test trade {trade_num}\", \n",
    "        req_order=req_open_order,\n",
    "        block_duplicates=True,\n",
    "        save_updates=True\n",
    "    )\n",
    "\n",
    "    trade_indexes.append(trade_index)\n",
    "\n",
    "trade_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data saving tests\n",
    "pm.save_to_json()\n",
    "#pm.load_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing trades test\n",
    "for trade_num in range(8*num_trades):\n",
    "    trade_index: int = random.randint(-5, 2*num_trades)\n",
    "\n",
    "    # Retrieve most recent bar\n",
    "    last_bar = Bars.reqBars(\n",
    "        conid = conid,\n",
    "        periods = 1,\n",
    "        period_unit = PeriodUnits.MONTH,\n",
    "        bar = 1,\n",
    "        bar_unit = BarUnits.HOUR,\n",
    "        outsideRth = False,\n",
    "    ).iloc[-1]\n",
    "\n",
    "    pm.closeTrade(\n",
    "        trade_index=trade_index,\n",
    "        bar=last_bar, \n",
    "        reason=f\"Test close trade at index {trade_index}\", \n",
    "        save_updates=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many trades still open\n",
    "sum((pm.trade_data[i].close_data is None) for i in range(len(pm.trade_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trades endpoint tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Requires 1 call with empty return before returning data, unless set to default!\n",
    "req_tr = cpwalib.trades()\n",
    "tr_df = pd.DataFrame(req_tr.json())\n",
    "\n",
    "# FIlter to desired columns and only recently executed trades, and then sort by time \n",
    "tr_df[[\"symbol\", \"trade_time\", \"execution_id\",\"side\", \"price\", \"order_ref\", \"net_amount\", \"size\", \"sec_type\", ]].sort_values(by=\"trade_time\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_tr.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timestamp(req_tr.json()[0]['trade_time_r'], unit=\"ms\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_pos = cpwalib.accountPositions(globals.accountID, 0)\n",
    "pos_df = pd.DataFrame(req_pos.json())\n",
    "req_pos.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_pos = cpwalib.accountPosition(globals.accountID, \"673277361\")\n",
    "req_pos.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backtesting and Live Trading Programs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Moved to bt.py in BT \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
